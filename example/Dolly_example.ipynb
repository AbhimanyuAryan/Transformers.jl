{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37ead7f",
   "metadata": {},
   "source": [
    "This is a tutorial of how to use Large Language Model (LLM) with [Transformers.jl](https://github.com/chengchingwen/Transformers.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8335e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Transformers, CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5f1f1",
   "metadata": {},
   "source": [
    "After loading the package, we need to setup the gpu. Currently multi-gpu is not supported. If your machine have multiple gpu devices, we can use `CUDA.devices()` to get the list of all device and use `CUDA.device!(device_number)` to specify the device we want to run our model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375362d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.DeviceIterator() for 8 devices:\n",
       "0. NVIDIA A100 80GB PCIe\n",
       "1. NVIDIA A100 80GB PCIe\n",
       "2. NVIDIA A100-PCIE-40GB\n",
       "3. Tesla V100-PCIE-32GB\n",
       "4. Tesla V100-PCIE-32GB\n",
       "5. Tesla V100S-PCIE-32GB\n",
       "6. Tesla V100-PCIE-32GB\n",
       "7. Tesla V100-PCIE-32GB"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1009e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA A100 80GB PCIe"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device!(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1d392",
   "metadata": {},
   "source": [
    "For demonstration, we disable the scalar indexing on gpu so that we can make sure all gpu calls are handled without performance issue. By setting `enable_gpu`, we get a `todevice` provided by Transformers.jl that will move data/model to gpu device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36df972b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "todevice (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.allowscalar(false)\n",
    "enable_gpu(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7fd2e",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to do use the [dolly-v2-12b (https://huggingface.co/databricks/dolly-v2-12b)](https://huggingface.co/databricks/dolly-v2-12b) in Julia. Dolly is an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use. It's based on the EleutherAI pythia model family and fine-tuned exclusively on a new, high-quality human generated instruction-following dataset [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k), crowdsourced among Databricks employees. They provide 3 model size: [dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b), [dolly-v2-7b](https://huggingface.co/databricks/dolly-v2-7b), and [dolly-v2-12b](https://huggingface.co/databricks/dolly-v2-12b). More information can be founded in [databricks' blogpost](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)\n",
    "\n",
    "\n",
    "The process should also work for other causal LM based model. With Transformers.jl, we can get the tokenizer and model by using the `hgf\"\"` macro or `HuggingFace.load_tokenizer`/`HuggingFace.load_model`. The required files like the model weights will be downloaded and managed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b404517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HGFGPTNeoXForCausalLM(\n",
       "  HGFGPTNeoXModel(\n",
       "    CompositeEmbedding(\n",
       "      token = Embed(5120, 50280),       \u001b[90m# 257_433_600 parameters\u001b[39m\n",
       "    ),\n",
       "    Chain(\n",
       "      Transformer<36>(\n",
       "        ParallelPreNorm2TransformerBlock(\n",
       "          SelfAttention(\n",
       "            CausalGPTNeoXRoPEMultiheadQKVAttenOp(base = 10000.0, dim = 32, head = 40, p = nothing),\n",
       "            GPTNeoXSplit(40, Dense(W = (5120, 15360), b = true)),  \u001b[90m# 78_658_560 parameters\u001b[39m\n",
       "            Dense(W = (5120, 5120), b = true),  \u001b[90m# 26_219_520 parameters\u001b[39m\n",
       "          ),\n",
       "          LayerNorm(5120, ϵ = 1.0e-5),  \u001b[90m# 10_240 parameters\u001b[39m\n",
       "          Chain(\n",
       "            Dense(σ = NNlib.gelu, W = (5120, 20480), b = true),  \u001b[90m# 104_878_080 parameters\u001b[39m\n",
       "            Dense(W = (20480, 5120), b = true),  \u001b[90m# 104_862_720 parameters\u001b[39m\n",
       "          ),\n",
       "          LayerNorm(5120, ϵ = 1.0e-5),  \u001b[90m# 10_240 parameters\u001b[39m\n",
       "        ),\n",
       "      ),\u001b[90m                  # Total: 432 arrays, \u001b[39m11_327_016_960 parameters, 72.859 KiB.\n",
       "      LayerNorm(5120, ϵ = 1.0e-5),      \u001b[90m# 10_240 parameters\u001b[39m\n",
       "    ),\n",
       "  ),\n",
       "  Branch{(:logit,) = (:hidden_state,)}(\n",
       "    EmbedDecoder(Embed(5120, 50280)),   \u001b[90m# 257_433_600 parameters\u001b[39m\n",
       "  ),\n",
       ") \u001b[90m                  # Total: 436 arrays, \u001b[39m11_841_894_400 parameters, 93.758 KiB."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Transformers.HuggingFace\n",
    "\n",
    "textenc = hgf\"databricks/dolly-v2-12b:tokenizer\"\n",
    "model = todevice(hgf\"databricks/dolly-v2-12b:ForCausalLM\") # move to gpu with `todevice` (or `Flux.gpu`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4be5b",
   "metadata": {},
   "source": [
    "We define some helper functions for the text generation. Here we are doing the simple greedy decoding. It can be replaced with other decoding algorithm like beam search. The `k` in `top_k_sample` decide the number of possible choices at each generation step. The default `k = 1` is simply `argmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc38fba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top_k_sample (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "using StatsBase\n",
    "\n",
    "function temp_softmax(logits; temperature = 1.2)\n",
    "    return softmax(logits ./ temperature)\n",
    "end\n",
    "\n",
    "function top_k_sample(probs; k = 1)\n",
    "    sorted = sort(probs, rev = true)\n",
    "    indexes = partialsortperm(probs, 1:k, rev=true)\n",
    "    index = sample(indexes, ProbabilityWeights(sorted[1:k]), 1)\n",
    "    return index\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f6860a",
   "metadata": {},
   "source": [
    "The main generation loop is defined as follows:\n",
    "\n",
    "1. The prompt is first preprocessed and encoded with the tokenizer `textenc`. The `encode` function return a `NamedTuple` where `.token` is the one-hot representation of our context tokens.\n",
    "2. At each iteration, we copy the tokens to gpu and feed them to the model. The model also return a `NamedTuple` where `.logit` is the predictions of our model. We then apply the greedy decoding scheme to get the prediction of next token. The token will be appended to the end of context tokens. The iteration stop if we exceed the maximum generation length or the predicted token is an end token.\n",
    "3. After the loop, we decode the one-hot encoding back to text tokens. The `decode` function convert the onehots to texts and also perform some post-processing to get the final list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41169a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_text (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Transformers.TextEncoders\n",
    "\n",
    "function generate_text(textenc, model, context = \"\"; max_length = 512, k = 1, temperature = 1.2, ends = textenc.endsym)\n",
    "    encoded = encode(textenc, context).token\n",
    "    ids = encoded.onehots\n",
    "    ends_id = lookup(textenc.vocab, ends)\n",
    "    for i in 1:max_length\n",
    "        input = (; token = encoded) |> todevice\n",
    "        outputs = model(input)\n",
    "        logits = @view outputs.logit[:, end, 1]\n",
    "        probs = temp_softmax(logits; temperature)\n",
    "        new_id = top_k_sample(collect(probs); k)[1]\n",
    "        push!(ids, new_id)\n",
    "        new_id == ends_id && break\n",
    "    end\n",
    "    return decode(textenc, encoded)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d26e1",
   "metadata": {},
   "source": [
    "We use the same prompt of dolly defined in [instruct_pipeline.py](https://huggingface.co/databricks/dolly-v2-12b/blob/main/instruct_pipeline.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a0c552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate(textenc, model, instruction; max_length = 512, k = 1, temperature = 1.2)\n",
    "    prompt = \"\"\"\n",
    "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction:\n",
    "    $instruction\n",
    "    \n",
    "    ### Response:\n",
    "    \"\"\"    \n",
    "    text_token = generate_text(textenc, model, prompt; max_length, k, temperature, ends = \"### End\")\n",
    "    gen_text = join(text_token)\n",
    "    println(gen_text)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f75981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain to me the difference between nuclear fission and fusion.\n",
      "\n",
      "### Response:\n",
      "Nuclear fission and fusion are both methods by which the nucleus of an atom splits and combines, releasing energy in the process. In nuclear fission, the nucleus is split into two or more smaller pieces. This releases a lot of energy in the form of heat and light, but the pieces are often unstable and will decay into smaller pieces over time. Nuclear fusion occurs when two or more nuclei combine to form a larger nucleus. This process releases less energy than nuclear fission but is more stable, and the energy can be captured and released more efficiently.\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "generate(textenc, model, \"Explain to me the difference between nuclear fission and fusion.\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
